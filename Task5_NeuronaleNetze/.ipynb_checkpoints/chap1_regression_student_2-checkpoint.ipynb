{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with Neural Networks\n",
    "\n",
    "Artificial neural networks are used to solve an extensive variety of problems. In this chapter we will focus on the problem of regression, since it is the easiest to begin with. We will explore the foundations of neural networks by using them to solve exemplary regression tasks. At first, the single artificial neuron is introduced. In the next step, activation functions are made familiar through interactive tasks. Finally, backpropagation is explained at the end of the chapter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Artificial Neuron in Theory\n",
    "\n",
    "\n",
    "An artificial neuron is a mathematical function that is inspired by the information processing of a biological neural cell. Each neuron $k$ accepts one or multiple values as inputs (X) and outputs one value (Y). It thereby performs a very simple mathematical operation:\n",
    "\n",
    "- **Weights** $w_{ki}$ multiply the input values\n",
    "- A **summation $v_{k}$** of the weighted inputs is calculated\n",
    "- A **constant value** $b_k$ is added to the sum (so called **Bias**)\n",
    "- An **activation function $ùúô$** is applied to the sum\n",
    "\n",
    "The resulting output of the activation function is the output (also called \"activation\") $y$ of the neuron. \n",
    "Even though this mechanism is very simple, a multitude of simple neurons is able to solve very complex problems.\n",
    "\n",
    "An artificial Neuron can be described as follows (1):\n",
    "\\begin{align}\n",
    "y & = \\phi\\sum_{n=0}^N {x_n \\cdot w_n}       \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;        (1)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "<img src=\"images/neural_network.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 1 - General artificial neuron\n",
    "</p>\n",
    "\n",
    "\n",
    "**Note:** An artificial Neuron is just an abstract concept. Even though we will create _neuron objects_ in this Jupyter Notebook for code reusability, a neuron is not bound to any specific shape. As long as we have a mathematical function that can be expressed in the Form of (1), we can view it as a \"Neuron\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Neuron in Practice\n",
    "For the sake of explanation, we will now examine a neuron with only one single input and without any activation function. This neuron is already able to model functions with the form (2). The neuron can be visualized as seen in Figure 2.\n",
    "\n",
    "\\begin{align}\n",
    "y & = w * x + b \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;        (2)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/single_neuron_no_activation.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 2 - Simple artificial neuron\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neuron class will just have one input, one weight and one bias. \n",
    "- Upon initialization, it will be connected to an interactive plot\n",
    "- Its weights and biases can be changed using the set_values method.\n",
    "- Its weights and biases can be polled using the get_weights/get_bias methods.\n",
    "- When the Neuron is changed, it notifies the interactive plot to redraw its output\n",
    "- It has a compute method that computes the activation based in the weight and input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to define a neuron class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "class SimpleNeuron:\n",
    "    def __init__(self, plot):\n",
    "        self.plot = plot #I am assigned the following plot\n",
    "        self.plot.register_neuron(self) #hey plot, remember me\n",
    "        \n",
    "    def set_values(self, weight, bias):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        self.plot.update() #hey plot, I have changed, redraw my output\n",
    "        \n",
    "    def get_weight(self):\n",
    "        return self.weight\n",
    "    \n",
    "    def get_bias(self):\n",
    "        return self.bias\n",
    "\n",
    "    def compute(self, x):\n",
    "        self.activation = self.weight * x + self.bias\n",
    "        return self.activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  The Problem of Regression\n",
    "In the task of regression analysis, a model function has to be found that matches a given set of data points N as **accurately** as possible. A commonly used metric for the accuracy of the approximation is the **least squares approach**. The vertical distance between each data point $(x_n|y_n)$ and the model function $m(x_n)$ is calculated by subtraction of the y-values of the data points with the predicted y-values of the model function (3).\n",
    "\n",
    "\\begin{align}\n",
    "d_n & = m(x_n)-y_n \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;        (3)\n",
    "\\end{align}\n",
    "\n",
    "The distances are then each squared and summed up. Since we want to compare the quality of an approximation with other approximations that have a different amount of data points, we also divide the sum by the total number of data points (_Mean Squared Error_). This will be our **Loss** function(4). Our goal is to keep this metric as low as possible, since the lower the loss, the better the approximation.\n",
    "\n",
    "\\begin{align}\n",
    "Loss & = \\frac{1}{n} \\sum_{n=0}^N {[m(x_n)-y_n]}^2 \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;        (4)\n",
    "\\end{align}\n",
    "\n",
    "If we have achieved an accurate regression, we can make **predictions** with it. We will train our neurons to match a given set of points and then use them to predict new points. That is, we will give the neuron new x-values and it will predict y-values.\n",
    "\n",
    "\n",
    "<img src=\"images/least_squares_explanation.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 3 - Distance to model function visualized\n",
    "</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a function \"loss\" that performs the operation (4). It will receive a neuron object and a set of points as arguments.\n",
    "- For each point that we give it, it first separates x and y-values. \n",
    "- It hands the neuron an x-value and asks the neuron to compute a prediction for the y-value. (see $m(x_n)$) \n",
    "- Then it subtracts the real y-value from the predicted y-value, as in operation (3), resulting in a distance\n",
    "- It then squares up the distance and accumulates the squared distances.  \n",
    "- In the last step, it divides the sum of squared distances by the amount of compared points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to define a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "def loss(neuron, points):\n",
    "    sum_squared_dist = 0\n",
    "\n",
    "    for point_x, point_y in zip(points[\"x\"], points[\"y\"]):  # zip merges both points[\"x\"] and points[\"y\"]\n",
    "\n",
    "        predicted_point_y = neuron.compute(point_x)\n",
    "        dist = point_y - predicted_point_y\n",
    "        squared_dist = dist ** 2\n",
    "        sum_squared_dist += squared_dist\n",
    "\n",
    "    loss = sum_squared_dist / len(points[\"y\"])\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing an Interactive Plot\n",
    "\n",
    "After importing the necessary libraries, we will set up an interactive plot class. It plots the output of a neuron by asking it to compute a set of x-values, which results in a set of predicted y-values that can be drawn on a plane. If the weight or bias of a neuron is changed, the neuron calls the \"redraw\" method of its plot to update it. The plot can also plot fixed points. Interactive sliders will be used to directly modify the weights and biases of neuron objects.\n",
    "\n",
    "**Note:** The plot classes are not part of the subject matter for this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells below to import libraries and define an interactive plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "import numpy as np\n",
    "import plotly.offline as plotly\n",
    "import plotly.graph_objs as go\n",
    "from ipywidgets import interact, Layout, HBox, FloatSlider\n",
    "import time\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "# an Interactive Plot monitors the activation of a neuron or a neural network\n",
    "class Interactive2DPlot:\n",
    "    def __init__(self, points, ranges, width=800, height=400, margin=dict(t=0, l=170), draw_time=0.05):\n",
    "        self.idle = True\n",
    "        self.points = points\n",
    "        self.x = np.arange(ranges[\"x\"][0], ranges[\"x\"][1], 0.1)\n",
    "        self.y = np.arange(ranges[\"y\"][0], ranges[\"y\"][1], 0.1)\n",
    "        self.draw_time = draw_time\n",
    "        self.layout = go.Layout(\n",
    "            xaxis=dict(title=\"x\", range=ranges[\"x\"], fixedrange=True),\n",
    "            yaxis=dict(title=\"y\", range=ranges[\"y\"], fixedrange=True),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            showlegend=False,\n",
    "            autosize=False,\n",
    "            margin=margin,\n",
    "        )\n",
    "        self.trace = go.Scatter(x=self.x, y=self.y)\n",
    "        self.plot_points = go.Scatter(x=points[\"x\"], y=points[\"y\"], mode=\"markers\")\n",
    "        self.data = [self.trace, self.plot_points]\n",
    "        self.plot = go.FigureWidget(self.data, self.layout)\n",
    "        # self.plot = plotly.iplot(self.data, self.layout,config={\"displayModeBar\": False})\n",
    "\n",
    "    def register_neuron(self, neuron):\n",
    "        self.neuron = neuron\n",
    "\n",
    "    def redraw(self):\n",
    "        self.idle = False\n",
    "        time.sleep(self.draw_time)\n",
    "        self.plot.data[0].y = self.neuron.compute(self.x)\n",
    "        self.idle = True\n",
    "\n",
    "    def update(self):\n",
    "        print(\"Loss: {:0.2f}\".format(loss(self.neuron, self.points)))\n",
    "        if self.idle:\n",
    "            thread = threading.Thread(target=self.redraw)\n",
    "            thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Train Neuron\n",
    "You are given a set of 3 points and one neuron to do a curve fit. Run the cell below.\n",
    "\n",
    "**Change the weight and bias of the neuron using the sliders to minimize the loss.**\n",
    "\n",
    "Hint: you can also change the sliders with the arrow keys on your keyboard after clicking on the slider\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='weight', layout=Layout(width='90%'), max=3.0, min=-3‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '1f107528-bfe3-4884-be1b-2edc4be3f393',\n",
       " ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not change\n",
    "points_lr = dict(x=[1, 2, 3], y=[1.5, 0.7, 1.2])\n",
    "ranges_lr = dict(x=[-4, 4], y=[-4, 4])\n",
    "\n",
    "linreg_plot = Interactive2DPlot(points_lr, ranges_lr)\n",
    "simple_neuron = SimpleNeuron(linreg_plot)\n",
    "\n",
    "slider_layout = Layout(width=\"90%\")\n",
    "\n",
    "\n",
    "interact(\n",
    "    simple_neuron.set_values, \n",
    "    weight=FloatSlider(min=-3, max=3, step=0.1, value = 0, layout=slider_layout),\n",
    "    bias=FloatSlider(min=-3, max=3, step=0.1, value = 0, layout=slider_layout)\n",
    ")\n",
    "\n",
    "linreg_plot.plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the optimal weight and bias combination?\n",
    "\n",
    "**Answer:** The optimal combination is weight = -0.2 and bias = 1.60, resulting in a loss of 0.10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a 3D-Plot\n",
    "We can see that searching for the lowest loss is a **parameter optimization problem**. For now, the problem can be solved manually, but if we want to use neural networks to solve more complex problems, we have to find a way to automate this process.\n",
    "\n",
    "The loss function is changed with both the specified weight and the specified bias. This relationship can be visualized three-dimensionally, which can give us further insight to construct an algorithm that solves the optimization problem. \n",
    "In this 3D-View, logarithmic scales are used to emphasize the topography. We will define a new function to compute the logarithmic loss for a set of points.\n",
    "\n",
    "The plot will be defined as follows:\n",
    "- the **Y axis** represents the bias\n",
    "- the **X axis** represents the weights \n",
    "- the **Z axis** (height) represents the corresponding loss value at a given weight/bias configuration. For illustration purposes, the logarithm of the MSE Loss is displayed.\n",
    "- the **black ball** represents the current weight/bias configuration. Its height represents the loss of that configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells below to define a 3D plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "def log_mse(neuron, points):\n",
    "    least_squares_loss = loss(neuron, points)\n",
    "    return np.log10(least_squares_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "class Interactive3DPlot:\n",
    "    def __init__(self, points, ranges, width=600, height=600, draw_time=0.1):\n",
    "        self.idle = True\n",
    "        self.points = points\n",
    "        self.draw_time = draw_time\n",
    "        self.threading = threading\n",
    "\n",
    "        self.range_weights = np.arange(  # Array with all possible weight values in the given range\n",
    "            ranges[\"x\"][0], ranges[\"x\"][1], 0.1\n",
    "        )\n",
    "        self.range_biases = np.arange(  # Array with all possible bias values in the given range\n",
    "            ranges[\"y\"][0], ranges[\"y\"][1], 0.1\n",
    "        )\n",
    "        self.range_biases_t = self.range_biases[:, np.newaxis]  # Bias array transposed\n",
    "        self.range_losses = []  # initialize z axis for 3D surface\n",
    "\n",
    "        self.ball = go.Scatter3d(  # initialize ball\n",
    "            x=[], y=[], z=[], hoverinfo=\"none\", mode=\"markers\", marker=dict(size=12, color=\"black\")\n",
    "        )\n",
    "\n",
    "        self.layout = go.Layout(\n",
    "            width=width,\n",
    "            height=height,\n",
    "            showlegend=False,\n",
    "            autosize=False,\n",
    "            margin=dict(t=0, l=0),\n",
    "            scene=dict(\n",
    "                xaxis=dict(title=\"weight\", range=ranges[\"x\"], autorange=False, showticklabels=True),\n",
    "                yaxis=dict(title=\"bias\", range=ranges[\"y\"], autorange=False, showticklabels=True),\n",
    "                zaxis=dict(title=\"log(MSE)\", range=ranges[\"z\"], autorange=True, showticklabels=False),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.data = [\n",
    "            go.Surface(\n",
    "                z=self.range_losses,\n",
    "                x=self.range_weights,\n",
    "                y=self.range_biases,\n",
    "                colorscale=\"Viridis\",\n",
    "                opacity=0.9,\n",
    "                showscale=False,\n",
    "                hoverinfo=\"none\",\n",
    "            ),\n",
    "            self.ball,\n",
    "        ]\n",
    "\n",
    "        self.plot = go.FigureWidget(self.data, self.layout)\n",
    "\n",
    "    def register_neuron(self, neuron):\n",
    "        self.neuron = neuron\n",
    "        self.calc_surface()\n",
    "\n",
    "    def calc_surface(self):  # height of 3d surface represents loss of weight/bias combination\n",
    "        self.neuron.weight = (  #instead of 1 weight and 1 bias, let Neuron have an array of all weights and biases\n",
    "            self.range_weights\n",
    "        )\n",
    "        self.neuron.bias = self.range_biases_t\n",
    "        self.range_losses = log_mse(  # result: matrix of losses of all weight/bias combinations in the given range\n",
    "            self.neuron, self.points\n",
    "        )\n",
    "        self.plot.data[0].z = self.range_losses\n",
    "\n",
    "    def update(self):\n",
    "        if self.idle:\n",
    "            thread = threading.Thread(target=self.redraw)\n",
    "            thread.start()\n",
    "\n",
    "    def redraw(self):  # when updating, only the ball is redrawn\n",
    "        self.idle = False\n",
    "        time.sleep(self.draw_time)\n",
    "        self.ball.x = [self.neuron.weight]\n",
    "        self.ball.y = [self.neuron.bias]\n",
    "        self.ball.z = [log_mse(self.neuron, self.points)]\n",
    "        self.plot.data[1].x = self.ball.x\n",
    "        self.plot.data[1].y = self.ball.y\n",
    "        self.plot.data[1].z = self.ball.z\n",
    "        self.idle = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "class DualPlot:\n",
    "    def __init__(self, points, ranges_3d, ranges_2d):\n",
    "        self.plot_3d = Interactive3DPlot(points, ranges_3d)\n",
    "        self.plot_2d = Interactive2DPlot(points, ranges_2d, width=400, height=500, margin=dict(t=200, l=30))\n",
    "\n",
    "    def register_neuron(self, neuron):\n",
    "        self.plot_3d.register_neuron(neuron)\n",
    "        self.plot_2d.register_neuron(neuron)\n",
    "\n",
    "    def update(self):\n",
    "        self.plot_3d.update()\n",
    "        self.plot_2d.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Train Neuron\n",
    "You are given the same set of 3 points and again one neuron to do a curve fit. Run the cell below.\n",
    "\n",
    "- **Change the weight and bias of the neuron using the sliders to minimize the loss**\n",
    "- **Observe all changes**\n",
    "\n",
    "**Note**: you can turn the 3D-Plot by clicking on it and moving your cursor, but you have to stay inside the widget with your cursor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='weight', layout=Layout(width='90%'), max=2.0, min=-2‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FigureWidget({\n",
       "    'data': [{'colorscale': 'Viridis',\n",
       "              'hoverinfo': 'none',\n",
       "      ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not change\n",
    "ranges_3d = dict(x=[-2.5, 2.5], y=[-2.5, 2.5], z=[-1, 2.5])  # set up ranges for the 3d plot\n",
    "plot_task2 = DualPlot(points_lr, ranges_3d, ranges_lr)  # create a DualPlot object to mange plotting on two plots\n",
    "neuron_task2 = SimpleNeuron(plot_task2)  # create a new neuron for this task\n",
    "\n",
    "interact(\n",
    "    neuron_task2.set_values,\n",
    "    weight=FloatSlider(min=-2, max=2, step=0.2, layout=slider_layout),\n",
    "    bias=FloatSlider(min=-2, max=2, step=0.2, layout=slider_layout),\n",
    ")\n",
    "\n",
    "HBox((plot_task2.plot_3d.plot, plot_task2.plot_2d.plot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What does the optimal weight and bias combination correspond to in the 3D Plot?_\n",
    "\n",
    "**Answer:** Since the z axis represents the loss which we try to minimize, the optimal weight and bias combination corresponds to the lowest point in the direction of z in the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the steepness of the valley at the point of optimal weight and bias combination?\n",
    "\n",
    "**Answer:** The optimal combination is the minimum in the plot, therefore the gradient is zero in that point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Activation Functions\n",
    "We can only go so far in approximating functions with a neuron that only has weights and biases, since all we can do is linear regression. Activation functions expand our capabilities by introducing an additional nonlinearity into the neuron. With it, we can model more complex functions. The most commonly used activation function nowadays is **ReLU**. It just outputs the input value, as long as it's greater than 0. If it's lower than zero, it outputs 0. We can conveniently describe this function by taking the maximum of the input value and of 0. The greater value of both will be chosen as the output (5).\n",
    "\n",
    "\\begin{align}\n",
    "f_{relu}(x) & = max(0,x)  \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;    (5)\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to define a ReLU function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "def relu(input_val):\n",
    "    return np.where(input_val > 0, input_val, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw a neuron with a ReLu activation function as follows:\n",
    "<img src=\"images/single_neuron_relu.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 5 - Neuron with ReLU activiation function visualized\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new class to implement this neuron in Python. We will inherit all properties of a neuron from SimpleNeuron.\n",
    "We only change the output by first feeding it through our ReLU function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Implement Activation Function\n",
    "Complete the code below by adding a relu function to the neuron, like in Figure 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluNeuron(SimpleNeuron): #inherit from SimpleNeuron class\n",
    "    \n",
    "    def compute(self, inputs):\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "        self.activation = relu(self.weight * inputs + self.bias)\n",
    "\n",
    "        # STUDENT CODE until HERE\n",
    "        return self.activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Task: Nonlinear Climate Control\n",
    "\n",
    "You find yourself as an engineer at the company \"ClimaTronics\". Your company wants to implement AI technology to regulate their new air conditioning system \"Perfect Climate 9000\". Even though the problem can be solved easily with conventional programming, the management department wants you to implement AI to attract investors. You have to fulfill the following requirements that are visualized in the datasheet excerpt:\n",
    "\n",
    "\n",
    "`The climate control shall remain off for temperatures under 25¬∞C. At a temperature of 30¬∞C, it shall reach 10% of its cooling power. Between 30¬∞C and 40¬∞C, the cooling power shall rise quadratically with the temperature. Cooling power shall reach its maximum at 40¬∞C.`\n",
    "<img src=\"images/datasheet.png\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below for to display a interactive plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='weight', layout=Layout(width='90%'), max=10.0, min=-‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '95a07f7d-fba5-4100-927a-a45b9df060aa',\n",
       " ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not change\n",
    "points_climate = dict(x=[25.0, 27.5, 30.0, 32.5, 35, 37.5, 40.0], y=[0.0, 2.0, 10.0, 23.7, 43, 68.7, 100.0])\n",
    "\n",
    "ranges_climate = dict(x=[-4, 45], y=[-4, 105])\n",
    "climate_plot = Interactive2DPlot(points_climate, ranges_climate)\n",
    "our_relu_neuron = ReluNeuron(climate_plot)\n",
    "\n",
    "interact(\n",
    "    our_relu_neuron.set_values,\n",
    "    weight=FloatSlider(min=-10, max=10, step=0.1, value=0, layout=slider_layout),\n",
    "    bias=FloatSlider(min=-300.0, max=200.0, step=1, value=0, layout=slider_layout),\n",
    ")\n",
    "\n",
    "climate_plot.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Questions\n",
    "\n",
    "\n",
    "**Question:** When setting the bias to 0.00, how does changing the weight affect the output function?\n",
    "\n",
    "**Answer:** With a bias of 0.00, the weight controls the steepness of the output function. For weights higher than 0, the steepness increases for x>0, for negative weights the steepness changes for x<0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How does changing the bias affect the output function?\n",
    "\n",
    "**Answer:** The bias controls the offset of the starting temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** When setting the weight to 1.00 and the bias to -10, at what temperature does the climate control start?\n",
    "\n",
    "**Answer:** It starts at 10¬∞C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** When setting the weight to 1.00 and the bias to -20, at what temperature does the climate control start?\n",
    "\n",
    "**Answer:** It starts at 20¬∞C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** When setting the weight to 2.00 and the bias to -20, at what temperature does the climate control start?\n",
    "\n",
    "**Answer:** It starts at 10¬∞C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What's the best weight/bias configuration that you could find?\n",
    "\n",
    "**Answer:** Within the given range the best combination is weight = 7.1, bias = -200 resulting in a loss of 50.71.\n",
    "\n",
    "If you change the range of the bias you can get a better result of 18.24 with the combination weight = 9.0, bias = -266."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Using just one neuron, we can easily understand and retrace the influence of weight and bias.\n",
    "But our one-neuron-approximation is not enough to closely approximate the needed quadratic relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##  Neural Networks\n",
    "\n",
    "The approximation can be improved by using multiple neurons. Instead of just one neuron for our approximation, we construct a neural network. We will use two ReLU neurons and one output neuron that will have weights as well. Now we are free how we want to weigh the result of the two ReLU neurons in the middle.\n",
    "\n",
    "### Hidden Layers\n",
    "In this neural network, the two neurons in the middle now represent a **hidden layer**. It is called hidden, because the calculations no longer have any concrete representation.\n",
    "\n",
    "In the last task, the weight and bias had an easily traceable influence on the output.\n",
    "But by adding more neurons, the relationship between each weight and bias with the output becomes untraceable.\n",
    "We obtain the weights and biases by simply adapting them until the result turns out to be correct. In this process, we quickly loose overview of what exactly we are calculating. It becomes very hard to untangle a neuron and describe its responsibility in the system. \n",
    "\n",
    "The input value is multiplied by the first weights and after adding biases and running it through the activation function the values are multiplied again by the second weights. Hidden layers can be stacked multiple times after one another. This gives room for multiple calculation steps, allowing more complex functions.\n",
    "\n",
    "Neural networks using at least one hidden layer have an interesting property: They can be used to approximate any continuous function. _(See \"Further Reading\")_\n",
    "\n",
    "<img src=\"images/hidden_layer.png\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a class for neural networks. The network will have four weights and two biases.\n",
    "\n",
    "**Note:** For the sake of simplicity and code reusability, we will treat neural networks the same way we treat individual neurons in the past examples. Remember that an artificial neuron is only a mathematical function? A whole neural network can be also fully described by just one single function, as is done here when calculating the activation. The neurons don't have to take the concrete shape of individual data objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to define a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, plot):\n",
    "        self.plot = plot #I am assigned the following plot\n",
    "        self.plot.register_neuron(self) #hey plot, remember me\n",
    "        \n",
    "    def set_config(self, w_i1, w_o1, b1, w_i2, w_o2, b2):\n",
    "        self.w_i1 = w_i1\n",
    "        self.w_o1 = w_o1\n",
    "        self.b1 = b1\n",
    "        self.w_i2 = w_i2\n",
    "        self.w_o2 = w_o2\n",
    "        self.b2 = b2\n",
    "        self.show_config()\n",
    "        self.plot.update()  # please redraw my output\n",
    "\n",
    "    def show_config(self):\n",
    "        print(\"w_i1:\", self.w_i1, \"\\t| \", \"w_o1:\", self.w_o1,\"\\n\")\n",
    "        print(\"b1:\", self.b1, \"\\t| \", \"w_i2:\", self.w_i2,\"\\n\")\n",
    "        print(\"w_o2:\", self.w_o2, \"\\t| \", \"b2:\", self.b2,\"\\n\")\n",
    "\n",
    "    def compute(self, x):\n",
    "        self.prediction = (relu(self.w_i1 * x + self.b1) * self.w_o1\n",
    "                         + relu(self.w_i2 * x + self.b2) * self.w_o2)\n",
    "        return self.prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "###  Task: Nonlinear Climate Control with Neural Network\n",
    "\n",
    "Run the cell below and adapt weights and bias to reach a better approximation of the desired curve than in the previous task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='w_i1', layout=Layout(width='90%'), max=10.0, min=-10‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': 'd16f728c-83b6-4b1c-8be1-083a3959ec0b',\n",
       " ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not change\n",
    "climate_plot_adv = Interactive2DPlot(points_climate, ranges_climate)\n",
    "our_neural_net = NeuralNetwork(climate_plot_adv)\n",
    "\n",
    "interact(\n",
    "    our_neural_net.set_config,\n",
    "    w_i1=FloatSlider(min=-10, max=10, step=0.1, layout=slider_layout),\n",
    "    w_o1=FloatSlider(min=-10, max=10, step=0.1,  layout=slider_layout),\n",
    "    b1=FloatSlider(min=-200.0, max=200.0, step=1,  layout=slider_layout),\n",
    "    w_i2=FloatSlider(min=-10, max=10, step=0.1, layout=slider_layout),\n",
    "    w_o2=FloatSlider(min=-10, max=10, step=0.1,  layout=slider_layout),\n",
    "    b2=FloatSlider(min=-200.0, max=200.0, step=1,layout=slider_layout),\n",
    ")\n",
    "climate_plot_adv.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the best configuration you could find? (Copy from above the plot)\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "w_i1: 1.1 \t|  w_o1: 6.3 \n",
    "\n",
    "b1: -37.0 \t|  w_i2: 4.5 \n",
    "\n",
    "w_o2: 1.0 \t|  b2: -124.0 \n",
    "\n",
    "Loss: 2.16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "We can conclude that the quadratic relationship can be better approximated by using additional weights and biases. Using two ReLU Neurons, we can create a function with two bends.\n",
    "However, the complexity of finding the optimal weights/biases increases drastically with each variable. The more powerful our neural networks should be, the harder the optimization becomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##  Backpropagation\n",
    "\n",
    "The solution to our optimization problems is called backpropagation. We can automate the process of adjusting weights and biases. In this example, we will turn back to the basics and use a simple neuron without an activation function. Backpropagation works by taking the partial derivatives of the loss function with respect to weight and bias. At each point, the bias and weight gradients points to the direction of higher loss. The magnitude of the gradient represents the amount of increase in loss for a given step length in that direction. \n",
    "\n",
    "Suppose we were to _maximize_ loss in Fig 6.: All we need to do is to follow the partial derivatives by adding them to our current weight/bias point. That means, decrease weight a lot (see the axes in Fig. 6), and decrease bias by some lesser amount, since it has less magnitude.\n",
    "\n",
    "However, because we want to go down, we will _subtract_ the gradient from out current point. This will move us closer to the minimum. In the next step, we are further down and the valley, but not close enough. So we just repeat the steps until we reach the minimum.\n",
    "\n",
    "Every step we take is called one **epoch**. (In this case _training steps_ and _epochs_ are equivalent). Because it is hard to determine whether the minimum is reached, we will specify the number of epochs before our descent and simply let the program run.\n",
    "\n",
    "If the magnitude of the gradients is too big, we will never reach a minimum. This is because our algorithm wants to move the ball too much at each step. It will oscillate around the minimum, but never arrive at it. In extreme cases, the movement even can oscillate up to infinty. To give us control over the amount of movement, the gradient is multiplied by a factor called **learning rate**. By setting it to an optimal value, we can prevent oscillations. However, if the learning rate is too small, the network will take forever to \"learn\", since the weights and biases are changing only very slowly.\n",
    "\n",
    "Number of epochs and learning rate are so called **hyperparameters**. They influence the training process but are not part of the network itself.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/backprop.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 6 - Partial derivatives of Loss function\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Backpropagation Plot\n",
    "We will create a new 3D-Plot that tracks our past weight/bias/loss values as we try to optimize the loss step by step. The black ball will leave a trace of its past values. Run the cell below to enable plotting the backpropagation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "plot_backprop = DualPlot(points_lr, ranges_3d, ranges_lr)\n",
    "trace_to_plot = go.Scatter3d(x=[], y=[], z=[], hoverinfo=\"none\", mode=\"lines\", line=dict(width=10, color=\"grey\"))\n",
    "\n",
    "plot_backprop.plot_3d.data.append(trace_to_plot)  # Expand 3D Plot to also plot traces\n",
    "plot_backprop.plot_3d.plot = go.FigureWidget(plot_backprop.plot_3d.data, plot_backprop.plot_3d.layout)\n",
    "plot_backprop.plot_3d.draw_time = 0\n",
    "\n",
    "\n",
    "def redraw_with_traces(plot_to_update, neuron, trace_list, points):  # executed every update step\n",
    "    plot_to_update.plot_3d.plot.data[2].x = trace_list[\"x\"]\n",
    "    plot_to_update.plot_3d.plot.data[2].y = trace_list[\"y\"]\n",
    "    plot_to_update.plot_3d.plot.data[2].z = trace_list[\"z\"]\n",
    "    plot_to_update.plot_3d.plot.data[1].x = [neuron.weight]\n",
    "    plot_to_update.plot_3d.plot.data[1].y = [neuron.bias]\n",
    "    plot_to_update.plot_3d.plot.data[1].z = [log_mse(neuron, points)]\n",
    "    plot_to_update.update()\n",
    "\n",
    "\n",
    "def add_traces(neuron, points, trace_list):  # executed every epoch\n",
    "    trace_list[\"x\"].extend([neuron.weight])\n",
    "    trace_list[\"y\"].extend([neuron.bias])\n",
    "    trace_list[\"z\"].extend([log_mse(neuron, points)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## DIY Backpropagation\n",
    "\n",
    "To do backpropagation, first you have to determine the partial derivatives of the loss function of the \"simple neuron\" with respect to weight and bias. After that, you have to figure out how to properly adjust the weights and biases to the gradient scaled to the learning rate.\n",
    "Down below at the end of the document you can verify your results by training. If you hit the benchmark, your algorithm is correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Determine the Gradient\n",
    "\n",
    "\n",
    "**Finish the function below by yourself.**\n",
    "\n",
    "There are multiple solutions to this, your algorithm may adjust the weight and bias in the right direction despite the gradient calculation being wrong.\n",
    "\n",
    "**Benchmark:** If you can reach a loss of 0.22 after 100 epochs and a learning rate of 0.01, your solution is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_neuron_loss_gradient(neuron, points):\n",
    "\n",
    "    gradient_sum = dict(weight=0, bias=0)\n",
    "    for point_x, point_y in zip(points[\"x\"], points[\"y\"]):  # gradient is calculated for each points and summed up\n",
    "        gradient_sum[\"weight\"] += (\n",
    "            # hint: point_x and point_y are the current point values\n",
    "            # STUDENT CODE HERE\n",
    "\n",
    "            2 * (neuron.get_weight() * (point_x) + neuron.get_bias() - point_y) * point_x          \n",
    "            \n",
    "            # STUDENT CODE until HERE\n",
    "        )\n",
    "\n",
    "        gradient_sum[\"bias\"] += (\n",
    "            # STUDENT CODE HERE     \n",
    "            \n",
    "            2 * (neuron.get_weight() * (point_x) + neuron.get_bias() - point_y)\n",
    "\n",
    "            # STUDENT CODE until HERE\n",
    "        )\n",
    "\n",
    "    gradient = dict(weight=gradient_sum[\"weight\"] / len(points[\"x\"]), bias=gradient_sum[\"bias\"] / len(points[\"x\"]))\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Adjust the Neuron\n",
    "After finding the gradient you have to adjust the weight and bias of the neuron, based on the partial derivatives and the learning rate. You have to verify your results by training the net down below.\n",
    "**Finish the function below by yourself.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_neuron(neuron, gradient):\n",
    "    # STUDENT CODE HERE\n",
    "    \n",
    "    neuron.weight -= learning_rate * gradient[\"weight\"]\n",
    "    neuron.bias -= learning_rate * gradient[\"bias\"]\n",
    "\n",
    "    # STUDENT CODE until HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "def train(neuron, points, trace_list):\n",
    "    redraw_with_traces(neuron.plot, neuron, trace_list, points)\n",
    "    for i in range(1, epochs + 1):  # first Epoch is Epoch no.1\n",
    "        add_traces(neuron, points, trace_list)\n",
    "        gradient = simple_neuron_loss_gradient(neuron, points)\n",
    "        adjust_neuron(neuron, gradient)\n",
    "\n",
    "        if i % redraw_step == 0:\n",
    "            print(\"Epoch:{} \\t\".format(i), end=\"\")\n",
    "            redraw_with_traces(neuron.plot, neuron_backprop, trace_list, points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Choose Hyperparameters and Train\n",
    "**Choose an optimal learning rate and number of epochs by trying out values and running the two cells below**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HBox(children=(FigureWidget({\n",
       "    'data': [{'colorscale': 'Viridis',\n",
       "              'hoverinfo': 'none',\n",
       "      ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STUDENT CODE HERE\n",
    "\n",
    "learning_rate = 0.16\n",
    "epochs = 100\n",
    "\n",
    "# STUDENT CODE until HERE\n",
    "\n",
    "redraw_step = 10 # update plot every n'th epoch. too slow? set this to a higher value (e.g. 100)\n",
    "\n",
    "neuron_backprop = SimpleNeuron(plot_backprop)\n",
    "HBox((plot_backprop.plot_3d.plot, plot_backprop.plot_2d.plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 18.45\n",
      "Loss: 18.45\n",
      "Epoch:10 \tLoss: 0.44\n",
      "Epoch:20 \tLoss: 0.20\n",
      "Epoch:30 \tLoss: 0.14\n",
      "Epoch:40 \tLoss: 0.12\n",
      "Epoch:50 \tLoss: 0.10\n",
      "Epoch:60 \tLoss: 0.10\n",
      "Epoch:70 \tLoss: 0.10\n",
      "Epoch:80 \tLoss: 0.09\n",
      "Epoch:90 \tLoss: 0.09\n",
      "Epoch:100 \tLoss: 0.09\n"
     ]
    }
   ],
   "source": [
    "#run this cell to test algorithm\n",
    "\n",
    "np.random.seed(4) # keep this for benchmarking, remove to play around\n",
    "\n",
    "neuron_backprop.set_values(  # set weight and bias randomly\n",
    "    (5 * np.random.random() - 2.5), (5 * np.random.random() - 2.5)\n",
    ")\n",
    "trace_list1 = dict(x=[], y=[], z=[])\n",
    "train(neuron_backprop, points_lr, trace_list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark:** If you can reach a loss of 0.22 after 100 epochs and a learning rate of 0.03, your solution is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Questions\n",
    "\n",
    "**only answer this after your algorithm has hit the benchmark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What happens when you set the learing rate to 0.18? Explain this behavior.\n",
    "\n",
    "**Answer:** The ball bounces between the valley but does't go down because the learning rate is to high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What happens when you set the learing rate to 0.182? Explain this behavior.\n",
    "\n",
    "**Answer:** With an even higher learning rate, the amplitude of the oscillation increases and the ball goes up instead of down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the fastest learning rate you could find? (Using the other benchmark criteria)\n",
    "\n",
    "**Answer:** With a learning rate of 0.16 we can reach a loss of 0.1 after 50 epochs and a loss of 0.09 after 80 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Further Reading: Neural Networks are Universal Function Approximators\n",
    "\n",
    "It can be mathematically proven that neural Networks can approximate any continuous function, as long as they have at least one hidden layer, use nonlinear activation functions, and use a sufficient (but finite) amount of hidden layer neurons. \n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/089360809190009T?via%3Dihub\n",
    "Kurt Hornik,\n",
    "Approximation capabilities of multilayer feedforward networks,\n",
    "Neural Networks,\n",
    "Volume 4, Issue 2,\n",
    "1991,\n",
    "Pages 251-257"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "479.492px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
