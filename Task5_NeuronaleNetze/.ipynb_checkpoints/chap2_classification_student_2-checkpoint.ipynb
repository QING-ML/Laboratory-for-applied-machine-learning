{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Neural Networks\n",
    "In this chapter, you will understand the workings of a classifier and manually train one that operates on a single value. You will improve the classifier step by step and learn fundamental concepts about classification as you go along.\n",
    "Finally, you will use automated backpropagation to train a multi layer neural network to emulate a logic gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. Examples are assigning a given email to the \"spam\" or \"non-spam\" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.). [1]\n",
    "\n",
    "A classification process requires a dataset that is split into different categories. A classifier can be trained on this dataset by learning the relationship between certain properties of the input data and the corresponding categories. \n",
    "To classify new data, the process is similar as in the chapter \"Regression\", however additional computational steps can be added depending on the application.\n",
    "A common classification problem that can be solved by neural networks is image recognition (seen in Figure 1).\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/neural_network_classification.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 1 - Image recognition by a neural network\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells below to import the needed libraries and define a ReLU and MSE Loss function and a SimpleNeuron Class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "import numpy as np\n",
    "from ipywidgets import interact, Layout, FloatSlider\n",
    "import plotly.offline as plotly\n",
    "import plotly.graph_objs as go\n",
    "import time\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "def relu(input_val):\n",
    "    return np.where(input_val > 0, input_val, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "def mean_squared_loss(predictions, solutions):\n",
    "    total_squared_loss = np.sum(np.subtract(predictions, solutions)**2) #np allows to handle both values and lists\n",
    "    mean_squared_loss = total_squared_loss/len(predictions)\n",
    "    return mean_squared_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "class SimpleNeuron:\n",
    "    def __init__(self, plot):\n",
    "        self.plot = plot\n",
    "        self.plot.register_neuron(self) #hey plot, remember me\n",
    "\n",
    "    def set_values(self, weight, bias):\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        self.plot.update() #hey plot, I have changed, redraw my output\n",
    "        \n",
    "    def get_weight(self):\n",
    "        return self.weight\n",
    "    \n",
    "    def get_bias(self):\n",
    "        return self.bias\n",
    "\n",
    "    def compute(self, x):\n",
    "        self.activation = np.dot(self.weight, x) + self.bias\n",
    "        return self.activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "# an Interactive Plot monitors the activation of a neuron or a neural network\n",
    "class Interactive2DPlot:\n",
    "    def __init__(\n",
    "        self, points_red, points_blue, ranges, loss_function=mean_squared_loss, loss_string=\"Loss\", width=800, height=400, margin=dict(t=0, l=170),\n",
    "        draw_time=0.1\n",
    "    ):\n",
    "        self.idle = True\n",
    "        self.points_red = points_red\n",
    "        self.points_blue = points_blue\n",
    "        self.draw_time = draw_time\n",
    "        self.loss_function = loss_function\n",
    "        self.loss_string = loss_string\n",
    "\n",
    "        self.x = np.arange(ranges[\"x\"][0], ranges[\"x\"][1], 0.01)\n",
    "        self.y = np.arange(ranges[\"y\"][0], ranges[\"y\"][1], 0.01)\n",
    "\n",
    "        self.layout = go.Layout(\n",
    "            xaxis=dict(title=\"Neck height in m\", range=ranges[\"x\"]),\n",
    "            yaxis=dict(title=\"y\", range=ranges[\"y\"]),\n",
    "            width=width,\n",
    "            height=height,\n",
    "            showlegend=False,\n",
    "            margin=margin,\n",
    "        )\n",
    "        self.trace = go.Scatter(x=self.x, y=self.y)\n",
    "\n",
    "        self.plot_points_red = go.Scatter(\n",
    "            x=points_red[\"x\"], y=points_red[\"y\"], mode=\"markers\", marker=dict(color='rgb(255, 0, 0)', size=10)\n",
    "        )\n",
    "        self.plot_points_blue = go.Scatter(\n",
    "            x=points_blue[\"x\"],\n",
    "            y=points_blue[\"y\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(color='rgb(0, 0, 255)', size=10, symbol=\"square\"),\n",
    "        )\n",
    "\n",
    "        self.plot_point_new = go.Scatter(\n",
    "            x=[], y=[], mode=\"markers\", marker=dict(size=20, symbol=\"star\", color='rgb(0,0,0)')\n",
    "        )\n",
    "\n",
    "        self.data = [self.trace, self.plot_points_red, self.plot_points_blue, self.plot_point_new]\n",
    "        self.plot = go.FigureWidget(self.data, self.layout)\n",
    "\n",
    "    def register_neuron(self, neuron):\n",
    "        self.neuron = neuron\n",
    "\n",
    "    def redraw(self):\n",
    "        self.idle = False\n",
    "        time.sleep(self.draw_time)\n",
    "        self.plot.data[0].y = self.neuron.compute(self.x)\n",
    "        self.idle = True\n",
    "\n",
    "    def update(self):\n",
    "        loss_red = self.loss_function(self.neuron.compute(self.points_red[\"x\"]), self.points_red[\"y\"])\n",
    "        loss_blue = self.loss_function(self.neuron.compute(self.points_blue[\"x\"]), self.points_blue[\"y\"])\n",
    "        print(self.loss_string,\": {:0.3f}\".format((loss_red + loss_blue) / 2))\n",
    "\n",
    "        if self.idle:\n",
    "            thread = threading.Thread(target=self.redraw)\n",
    "            thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Regression to Classification\n",
    "\n",
    "##  Linear Regression\n",
    "\n",
    "You find yourself working on a farm with sheep and llamas grazing in seperate enclosures. However, last night the shepard forgot to close the gate between the two enclosures. The llamas and sheep now are mixed and have to be seperated again. You immediately come up with a machine learning based solution to separate the sheep from the llamas again: You assume that llamas can be distinguished from sheep by measuring the distance from the top of their head to their spine, since llamas have significantly longer necks. Using a LIDAR scanner, neck heights will be measured autonomously and the animals will be seperated using a food enticement and an electronic turnstile that only lets llamas through.\n",
    "\n",
    "\n",
    "<img src=\"images/neck_heights.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 2 - Concept of neck height measurement\n",
    "</p>\n",
    "\n",
    "To collect sample data, you go out on the field with a measuring tape and measure the neck heights of some sheep and llamas. You specify two categories: '0' for sheep and '1' for llamas. (See table 1)\n",
    "\n",
    "Most llamas are grown up and have long necks, but there are also some young llamas with smaller necks, but since their necks are still longer than the sheeps' you figure that this won't be a problem.\n",
    "\n",
    "|  Animal | Neck height  | Category  |\n",
    "|---------|--------------|-----------|\n",
    "| Sheep #1| 0.05m        |0          |\n",
    "| Sheep #2| 0.08m        |0          |\n",
    "| Sheep #3| 0.13m        |0          |\n",
    "| Sheep #4| 0.17m        |0          |\n",
    "| Sheep #5| 0.20m        |0          |\n",
    "| Llama #1| 0.35m        |1          |\n",
    "| Llama #2| 0.68m        |1          |\n",
    "| Llama #3| 0.74m        |1          |\n",
    "| Llama #4| 0.83m        |1          |\n",
    "| Llama #5| 0.95m        |1          |\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "    Table. 1 - Your data mining results\n",
    "</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Linear Regression Neuron\n",
    "For the sake of simplicity, you start by using a single neuron as a classifier. Run the two cells below to define the data mining points and to display a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "points_sheep = dict(\n",
    "              x=[ 0.05, 0.08, 0.13, 0.17, 0.20],\n",
    "              y=[ 0, 0, 0, 0, 0]\n",
    "             )\n",
    "\n",
    "points_llamas = dict(\n",
    "              x=[ 0.35, 0.68, 0.74, 0.83, 0.95],\n",
    "              y=[ 1,  1, 1, 1, 1]\n",
    "             )\n",
    "\n",
    "ranges = dict(x=[-0.1, 1.25], y=[-0.5, 1.4])\n",
    "slider_layout = Layout(width=\"90%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b392aecaef374952a6ca02bf4d4e1d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='weight', layout=Layout(width='90%'), max=4.0, min=-2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38946845822240aeacf3e06ac9ed5e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '56d117cb-42c4-4085-a60f-11486da06d5f',\n",
       " …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not change\n",
    "plot1 = Interactive2DPlot(points_sheep, points_llamas, ranges, loss_string=\"Mean Squared Loss\")\n",
    "neuron1 = SimpleNeuron(plot1)\n",
    "\n",
    "interact(\n",
    "    neuron1.set_values,\n",
    "    weight=FloatSlider(min=-2, max=4, step=0.1, layout = slider_layout),\n",
    "    bias=FloatSlider(min=-1, max=1, step=0.1, layout = slider_layout),\n",
    ")\n",
    "\n",
    "plot1.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**:\n",
    "What is the optimal weight and bias combination in the plot above?\n",
    "\n",
    "**Answer:** The optimal weight and bias combination is weight = 1.3 and bias = 0.00, resulting in a mean squared loss of 0.053."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Defining a Classifier\n",
    "Now we want to use our trained neuron to classify new neck heights. To do that, we have to write a program that takes in a neck height and outputs what the trained neuron thinks about it. The classifier will also plot the new neck height. Run the box below to get the values from the task before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Loss : 0.500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588fe1da0e6f4fb7af4a82927bec230d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': '1d76efd2-0fac-432d-8d32-f423a5e02ab0',\n",
       " …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not change\n",
    "#a duplicate of plot 1, so you don't have to scroll\n",
    "plot2 = Interactive2DPlot(points_llamas, points_sheep, ranges, loss_string=\"Mean Squared Loss\") \n",
    "neuron2 = SimpleNeuron(plot2)\n",
    "neuron2.set_values(neuron1.get_weight(), neuron1.get_bias()) #get your values from last task\n",
    "\n",
    "plot2.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Implement a Classifier\n",
    "Complete the python code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 0.0\n"
     ]
    }
   ],
   "source": [
    "new_neck_height = 0.35  #this shall be varied to answer the questions below\n",
    "\n",
    "#classification_result = ??\n",
    "\n",
    "#STUDENT CODE HERE\n",
    "\n",
    "classification_result = neuron2.compute(new_neck_height)\n",
    "\n",
    "#STUDENT CODE until HERE\n",
    "\n",
    "plot2.plot.data[3].x = [new_neck_height] #update plot\n",
    "plot2.plot.data[3].y = [classification_result]\n",
    "\n",
    "print(\"Result:\", classification_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "What classification value does the smallest llama have? (run the cell above and change new_neck_height)\n",
    "\n",
    "\n",
    "**Answer:** The smallest llama with a neck height of 0.35m has a classification value of 0.455."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "What classification value does an animal with a neck height of 0.1m have?\n",
    "\n",
    "\n",
    "**Answer:** ~0.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "What classification value does an animal with a neck height of 0.9m have?\n",
    "\n",
    "\n",
    "**Answer:** ~1.17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "Why is the classification value continuous, even though the training data had only two discrete values?\n",
    "\n",
    "\n",
    "**Answer:** The output of artificial neurons is always a continuous function. The contious categories in between 0 and 1 are a generalization based on linear regression made by the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "How do you interpret this continuous classification value? Try to describe it in a few words.\n",
    "\n",
    "\n",
    "**Answer:** The continuous value shows the probability of the algorithm being right in its classification result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "You want to use your continuous result to create a discrete \"llama\" or \"sheep\" classifier. The decision should be approximately just as sensitive towards llamas as to sheep.\n",
    "What threshold y-value would you choose?\n",
    "\n",
    "\n",
    "\n",
    "**Answer:** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "You want to add more data to your model to improve its performance. As you collect more data, you find a very small llama with a neck height of 0.25m in your dataset. After you train your model on the new data, your discrete classifier decides that this small llama is a sheep. What is the problem with applying thresholds to linear regression models for classification?\n",
    "\n",
    "\n",
    "**Answer:** The problem with applying thresholds is that the algorithm simply classifies by a fixed value that doesn't consider exceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Logistic Regression\n",
    "\n",
    "In machine learning, the go-to assumption for an unknown two-class probability distribution is a logistic distribution.[2]\n",
    "Its cumulated function is the logistic function, of which the sigmoid function is the most used special case. (See Fig 3.)\n",
    "The sigmoid function allows for a model that approximatly describes most natural occuring probability distributions.[3] (Further reading: see section \"Further Reading\" at the end of document)\n",
    "<img src=\"images/sigmoid.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 3 - Sigmoid function\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to define a sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Complete Code and Train Neuron\n",
    "\n",
    "\n",
    "Change the SigmoidNeuron class below to apply a sigmoid function to the final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidNeuron(SimpleNeuron): #inheriting from SimpleNeuron, \n",
    "                                   #all functions stay the same unless they are specified here\n",
    "\n",
    "    def compute(self, x):\n",
    "        # STUDENT CODE HERE\n",
    "        \n",
    "        self.activation = sigmoid(np.dot(self.weight, x) + self.bias)\n",
    "\n",
    "        # STUDENT CODE until HERE\n",
    "        return self.activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ffe01141db4b88b0f716922bc5fdb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='weight', layout=Layout(width='90%'), max=200.0, min=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3424aea92c7249d8a720104c788a1aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'type': 'scatter',\n",
       "              'uid': 'bce6e84e-f455-4689-bc95-720bff97d1f1',\n",
       " …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not change\n",
    "classification_plot_sig = Interactive2DPlot(points_llamas, points_sheep, ranges, loss_string=\"Mean Squared Loss\")\n",
    "\n",
    "our_sig_neuron = SigmoidNeuron(classification_plot_sig)\n",
    "\n",
    "interact(\n",
    "    our_sig_neuron.set_values,\n",
    "    weight=FloatSlider(min=-50, max=200, step=0.1, layout = slider_layout),\n",
    "    bias=FloatSlider(min=-50, max=50, step=0.1, layout = slider_layout),\n",
    ")\n",
    "\n",
    "classification_plot_sig.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "What is an optimal weight and bias combination?\n",
    "\n",
    "\n",
    "**Answer:** The optimal comibnation is weight = 40.60 and bias = -11.30 with a Mean Squared Loss of 0.00."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "What advantage does a classifier have in general that also outputs a probability compared to a classifier that just outputs a binary yes/no value? (a few words)\n",
    "\n",
    "\n",
    "**Answer:** We not only know the result but also how sure we can be that the given result is right. With that information we can correct wrong classifications of exceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "Give one example how we can use the additional probability information to increase the accuracy of our seperation process\n",
    "\n",
    "\n",
    "**Answer:** In cases where the classifier is uncertain (classification value around 0.5) we can sort out the animal and classify it by ourselves or we can take an additional information about the object into account if the classifier is uncertain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy/Logarithmic Loss:\n",
    "The most common loss function for classification is cross entropy loss, also called logarithmic loss. (In the context of machine learning, they are equal). In the special case of two categories, the loss is called binary cross entropy. The binary cross entropy loss between an actual data value $y$ and a predicted value $p$ is calculated as follows:\n",
    "\n",
    "\\begin{align}\n",
    "−[y \\cdot log(p)+(1−y)\\cdot log(1−p)]\n",
    "\\end{align}\n",
    "\n",
    "In this manner, the average of all data points is calculated.\n",
    "It turns out that the derivative of a logarithmic loss using one hot encoding (explained below) is just the solution vector subtracted by the network output, which makes it very easy to work with.\n",
    "**Note** Cross entropy loss can only be used, if the output values are between 0 and 1.\n",
    "\n",
    "<img src=\"images/cross_entropy.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "        Fig. 4 - Logarithmic / cross entropy loss function\n",
    "\n",
    "</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Calculate Squared and Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "What are the the squared loss and cross entropy loss results for the following predictions? Copy the table and fill out the ??? as an answer below.  Use the cell below for calculations.\n",
    "\n",
    "\n",
    "| Input         | Llama Probability  |      Squared Loss    | Cross Entropy Loss   |\n",
    "|---------------|--------------------|----------------------|----------------------|\n",
    "|    llama(1)   | 0.99               |0.0001                |0.0101                |\n",
    "|    sheep(0)   | 0.6                |0.3600                |0.9163                |\n",
    "|    sheep(0)   | 0.95               |0.9025                |2.9957                |\n",
    "|    sheep(0)   | 0.999999           |1.0000                |13.8155               |\n",
    "\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "def cross_entropy_loss(predictions,solutions):\n",
    "    predictions += 1e-15 #in order to prevent log(0)\n",
    "    total_loss = np.sum(-(solutions*np.log(predictions)+(1-solutions)*np.log(1-predictions)))\n",
    "    avg_loss = total_loss/len(predictions)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared loss: 1.0000\n",
      "cross entropy loss: 13.8155\n"
     ]
    }
   ],
   "source": [
    "predicted = np.array([0.999999]) #insert here\n",
    "actual = np.array([0]) #insert here\n",
    "\n",
    "\n",
    "print(\"mean squared loss: {:0.4f}\".format(mean_squared_loss(predicted,actual)))\n",
    "print(\"cross entropy loss: {:0.4f}\".format(cross_entropy_loss(predicted,actual)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "How do the goals of regression and classification generally differ?\n",
    "\n",
    "**Answer:** The goal of regression is a continuous output, the goal of classification is a discrete output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "Why do you think cross entropy loss is better suited for classification training algorithms?\n",
    "\n",
    "**Answer:** The cross entropy loss is better suited because it is more sensitive. This is important for the classification algorithm, because it is the only way to check our results, since the output is a discrete value and doesn't give us any information about the probability of the result being right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "To do classification, categories have to be represented in a way that the classifier can process. Neural networks cannot understand categories directly and need a numeric representation.\n",
    "\n",
    "### Disadvantages of Integer Encoding\n",
    "\n",
    "In the llama classifier, llamas were assigned the value $1$ and sheep the value $0$. One single output neuron would \"fire\", if a llama was found, and not fire, if a sheep was found. This type of representing categories is called **integer** or **label encoding**\n",
    "\n",
    "This works reasonably well for binary classification, but what if we want to distinguish between sheep, llamas and shepherd dogs?\n",
    "Doing this with just one output neuron would result in complications: \n",
    "- Dogs would need a label that is numerically higher or lower (for example $2$), implying an order (Dogs > Llamas) where there actually is none.\n",
    "- it would be necessary to interpret three different states out of one output neuron value\n",
    "\n",
    "Another disadvantage can be seen in the next question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "Suppose the encodings are: 0 for sheep, 1 for llamas and 2 for dogs.\n",
    "You classified 5 sheep and 5 dogs today. You want your classifier to output the average classification for today. What will the classifier say?\n",
    "\n",
    "**Answer:** Since the mean of those classified results is 1, the average classification for today would be llama."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition of One-Hot Encoding\n",
    "\n",
    "The solution looks like this:\n",
    "\n",
    "| Input         | One Hot Encoding  | \n",
    "|---------------|--------------------|\n",
    "|    sheep   | [1,0,0]                |\n",
    "|    llama   | [0,1,0]               |\n",
    "|    dog     | [0,0,1]           |\n",
    "\n",
    "\n",
    "\n",
    "The length of the representation vector is always equal to the amount of categories. Only one element of the vector is 1 for each category (\"one-hot\").\n",
    "Using this encoding, we can conveniently use 3 output neurons for 3 different categories, so that the activation of each output neuron represents the classification score for that category.\n",
    "\n",
    "###  Limits of One-Hot Encoding\n",
    "One-hot encoding is not an unimprovable solution to represent categories, but rather another tool in the box that happens to work well for many problems, but not for all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "Suppose you would like to train a speech recognition neural network that can classify all English words contained in the Oxford English Dictionary. It does not need to classify whole sentences, just single words. What would be a problem using one-hot encoding?\n",
    "\n",
    "**Answer:** You would need as many elements in the representation vector as there are words in the Oxford English Dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "The sigmoid function works fine for a \"yes or no\" problem. But more often than not we want to distinguish between more than two categories. For that, we need a function that takes in **multiple** neuron activations from the last layer of a network and outputs a **probability vector** containing the probabilities for each category.\n",
    "The key: Each input of this function is normalized by the other inputs such that the sum of the output vector is always 1. Figure 3 shows an example network.\n",
    "\n",
    "We can realize a softmax function by taking each element $x_i$ of the input vector, calculating $exp(x_i)$ and then normalizing this value by dividing it by the sum of the $exp$ results of all single input vector elements.\n",
    "\\begin{align}\n",
    "\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "<img src=\"images/softmax_example_network.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 3 - Sigmoid function\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "In \"logistic regression\", we also obtained a probability by applying a sigmoid function on the last layers' output.\n",
    "Why can't we apply a sigmoid function on each output neuron of this network instead of a softmax and get a probability vector?\n",
    "\n",
    "\n",
    "**Answer:** If we were to apply a sigmoid function on each neuron we would only recieve separate probabilities for each output but we wouldn't get the probabilities for each output in relation to all other outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Automated Classification Training\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We already have explored automated training using backpropagation in the last chapter. We had one set of points that we had to fit a function as close as possible. The task is similar for classification training. However instead of y-coordinates for points, we now have discrete categories.\n",
    "\n",
    "In task 1, you were given a set of neck lengths and the correspoding categories (see table 1). In the field of machine learing, this dataset is called __training data__. It specifies the behaviour that the neural net should have. We will use backpropagation to adjust the weights and biases of the network over and over again until the network outputs the same values to a given set of inputs as in the training data. During backpropagation, the network is figuratively \"learning\" the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Realizing an XOR Gate with a Neural Network\n",
    "\n",
    "You find yourself working as an engineer at a major electronic component manufacturing company. Your company wants to produce the first XOR gate chip that runs on artificial intelligence. You are given the training data in the form of a truth table:\n",
    "\n",
    "\n",
    "| Input 1| Input 2  | Output    |\n",
    "|--------|----------|-----------|\n",
    "|    0   | 0        |0          |\n",
    "|    0   | 1        |1          |\n",
    "|    1   | 0        |1          |\n",
    "|    1   | 1        |0          |\n",
    "\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "    Table. 2 - XOR Truth table\n",
    "</p>\n",
    "\n",
    "\n",
    "In this task we will make use of arrays and matrices to ease the handling of the data and the network parameters. We will also utilize a neural network without biases in order to make the algorithm as simple as possible.\n",
    "The training data consists of a 2D Array of all possible input states and a 1D Array of all corresponding outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task : Create Training Data\n",
    "\n",
    "A training set consists of an input set and a solution set. During supervised training, the network is adjusted until its predictions to the input set match the corresponding predetermined solutions.\n",
    "Complete the training data below using the truth table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xor_input_set = np.array(tablegoeshere)\n",
    "#xor_solution_set = np.array(tablegoeshere)\n",
    "\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "xor_input_set = np.array([[0, 0],\n",
    "                          [0, 1],\n",
    "                          [1, 0],\n",
    "                          [1, 1]])\n",
    "\n",
    "xor_solution_set = np.array([[0],\n",
    "                             [1],\n",
    "                             [1],\n",
    "                             [0]])\n",
    "\n",
    "# STUDENT CODE until HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Network\n",
    "Next, the Network has to be defined and initialized. For this task, we use a network with 3 hidden neurons (see Figure 4).\n",
    "\n",
    "<img src=\"images/3x2_xor_network.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 4 - Neural Network \n",
    "</p>\n",
    "\n",
    "We define $w_{01}, w_{02}, w_{03}, w_{10}, w_{11}, w_{12}$ all at once by just defining a 2x3 weight matrix $w_{l1}$ and do the same for $w_{l2}$. The matrices will be initialized with values between -1 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to define a neural network class that is depicted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.hl_sum = [0, 0, 0]\n",
    "        self.hl_activation = [0, 0, 0]\n",
    "        self.ol_sum = [0]\n",
    "        self.prediction = 0\n",
    "        self.b = 0\n",
    "        self.w_i = np.zeros((2, 3))\n",
    "        self.w_o = np.zeros((3, 1))\n",
    "        \n",
    "    def set_conf(self, w_i, w_o, b):  # w_i and w_o are matrices here\n",
    "        self.w_i = w_i\n",
    "        self.w_o = w_o\n",
    "        self.b = b\n",
    "\n",
    "    def get_conf(self):\n",
    "        configuration = dict();\n",
    "        configuration['w_i'] = self.w_i\n",
    "        configuration['w_o'] = self.w_o\n",
    "        configuration['b'] = self.b\n",
    "        return configuration\n",
    "\n",
    "    def get_ex(self):\n",
    "        excitations = dict();\n",
    "        excitations['hl_sum'] = self.hl_sum\n",
    "        excitations['hl_activation'] = self.hl_activation\n",
    "        excitations['ol_sum'] = self.ol_sum\n",
    "        return excitations\n",
    "    \n",
    "    \n",
    "    def show_conf(self):\n",
    "        print(\"weight matrix w_i:\")\n",
    "        print(self.w_i)\n",
    "        print(\"\\nweight matrix w_o:\")\n",
    "        print(self.w_o)\n",
    "        print(\"Bias\")\n",
    "        print(self.b)\n",
    "\n",
    "    def compute(self, input_set):\n",
    "        self.hl_sum = input_set.dot(self.w_i)\n",
    "        #Student Code \n",
    "        self.hl_activation = relu(self.hl_sum) \n",
    "        self.ol_sum = relu(self.hl_activation).dot(self.w_o) + self.b\n",
    "        self.prediction = sigmoid(self.ol_sum)\n",
    "\n",
    "        return self.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "logic_gate_net = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "def initialize_network(net):\n",
    "    #np.random.seed(3)\n",
    "    weight_matrix_i = np.random.rand(2,3)  # a 2x3 matrix of weights\n",
    "    weight_matrix_o = np.random.rand(3,1)  # a 3x1 matrix of weights\n",
    "    bias = np.random.randn()\n",
    "    net.set_conf(weight_matrix_i,weight_matrix_o,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight matrix w_i:\n",
      "[[0.38762955 0.66531803 0.66662743]\n",
      " [0.21083038 0.56962714 0.43640799]]\n",
      "\n",
      "weight matrix w_o:\n",
      "[[0.32496836]\n",
      " [0.33371356]\n",
      " [0.21101071]]\n",
      "Bias\n",
      "1.0301343734962858\n"
     ]
    }
   ],
   "source": [
    "# do not change\n",
    "initialize_network(logic_gate_net) #just a test initialization to illustrate the weight matrices\n",
    "logic_gate_net.show_conf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Training Process\n",
    "Finally, run the cells below to implement a backpropagation algorithm. Try to understand the code. See Fig. 4 for explanation of the variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not change\n",
    "def sigmoid_prime(x): #the derivative of sigmoid\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# do not change\n",
    "def train(net, input_set, solution_set, learning_rate, epochs):\n",
    "    for t in range(epochs):\n",
    "        # Forward pass: compute predicted solution_set\n",
    "        predictions = net.compute(input_set)\n",
    "        # Compute and print loss\n",
    "        log_loss = cross_entropy_loss(predictions, solution_set)\n",
    "        \n",
    "        if (t % 5 == 0):  # only output every 5th epoch\n",
    "            print(\"Loss after Epoch {}: {:0.4f}\".format(t, log_loss))\n",
    "\n",
    "        #unravel variables here for readability\n",
    "        ol_sum = net.get_ex()['ol_sum']\n",
    "        hl_activation = net.get_ex()['hl_activation']\n",
    "        hl_sum = net.get_ex()['hl_sum']\n",
    "        w_i = net.get_conf()['w_i']\n",
    "        w_o = net.get_conf()['w_o']\n",
    "        b = net.get_conf()['b']\n",
    "        \n",
    "        # Backpropagation to compute gradients of w_i and w_o with respect to loss\n",
    "        # start from the loss at the end and then work towards the front\n",
    "        grad_ol_sum = sigmoid_prime(ol_sum) * (predictions - solution_set)\n",
    "        grad_w_o = hl_activation.T.dot(grad_ol_sum)  # Gradient of Loss with respect to w_o\n",
    "        grad_hl_activation = grad_ol_sum.dot(w_o.T)  # the second layer's error\n",
    "        grad_hl_sum = hl_sum.copy()  # create a copy to work with\n",
    "        grad_hl_sum[hl_sum < 0] = 0  # the derivate of ReLU\n",
    "        grad_w_i = input_set.T.dot(grad_hl_sum * grad_hl_activation)  #\n",
    "\n",
    "        updated_weight_matrix_i = w_i - learning_rate * grad_w_i\n",
    "        updated_weight_matrix_o = w_o - learning_rate * grad_w_o\n",
    "        updated_bias = b - learning_rate * grad_ol_sum.sum()\n",
    "        net.set_conf(updated_weight_matrix_i, updated_weight_matrix_o,\n",
    "                       updated_bias)  # Apply updated weights to network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Choose Hyperparameters and Train\n",
    "Choose an optimal learning rate and number of epochs by trying out values and running the cell below.\n",
    "\n",
    "If your training data was correct, the network should be ready for use after training.\n",
    "A successfull training should result in a loss < 0.02.\n",
    "\n",
    "**Hint**:\n",
    "Press Shift+Enter on the cell below and then the \"up\" arrow key to repeat the training easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after Epoch 0: 0.8401\n",
      "Loss after Epoch 5: 0.7495\n",
      "Loss after Epoch 10: 0.7910\n",
      "Loss after Epoch 15: 0.7785\n",
      "Loss after Epoch 20: 0.9737\n",
      "Loss after Epoch 25: 0.4810\n",
      "Loss after Epoch 30: 0.4804\n",
      "Loss after Epoch 35: 0.4783\n",
      "Loss after Epoch 40: 0.4579\n",
      "Loss after Epoch 45: 0.4138\n",
      "Loss after Epoch 50: 0.4352\n",
      "Loss after Epoch 55: 3.1481\n",
      "Loss after Epoch 60: 3.2381\n",
      "Loss after Epoch 65: 3.3816\n",
      "Loss after Epoch 70: 3.4479\n",
      "Loss after Epoch 75: 3.4913\n",
      "Loss after Epoch 80: 3.5236\n",
      "Loss after Epoch 85: 3.5492\n",
      "Loss after Epoch 90: 3.5705\n",
      "Loss after Epoch 95: 3.5886\n"
     ]
    }
   ],
   "source": [
    "#learning_rate = ??\n",
    "#epochs = ??\n",
    "# STUDENT CODE HERE\n",
    "\n",
    "learning_rate = 10\n",
    "epochs = 100\n",
    "\n",
    "# STUDENT CODE until HERE\n",
    "\n",
    "initialize_network(logic_gate_net) #initialize again so you can just run this box and train a new network\n",
    "train(logic_gate_net, xor_input_set, xor_solution_set,learning_rate,epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "Why are the losses different each time you run the cell?\n",
    "\n",
    "**Answer:** Because the initialization of the weight and bias is random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "What is a good learning rate that reaches a loss < 0.02 in < 100 epochs most of the time?\n",
    "\n",
    "**Answer:** 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Classification Test\n",
    "Run the cell below and change the sliders and do a validation check on your logic gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61e6843c290408f8be00755eae9bbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='input1', layout=Layout(width='22%'), max=1.0, step=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not change\n",
    "def change(input1, input2):\n",
    "    input_vector = np.array([input1 * 1, input2 * 1])     # converting bool to float\n",
    "    prediction = logic_gate_net.compute(input_vector)\n",
    "    print(\"\\t input: {} \\t \\t output: {:0.9f}\".format(input_vector, prediction[0]))\n",
    "\n",
    "interact(\n",
    "    change,\n",
    "    input1=FloatSlider(min=0, max=1, step=1, layout=Layout(width=\"22%\")),\n",
    "    input2=FloatSlider(min=0, max=1, step=1, layout=Layout(width=\"22%\")),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Continuous Input Test\n",
    "\n",
    "Change the sliders and observe the changes when the input is varied continuously instead of binary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c1a391bc364b88a7fae4f4d1915fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='input1', max=1.0), FloatSlider(value=0.0, descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(change, input1=0.0, input2=0.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "What can you observe when changing the sliders? How would you describe the general relationship between the two inputs and the output (a few words)\n",
    "\n",
    "**Answer:** If the inputs are same, the output is zero. If the inputs are different from each other, the output is one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "Change the sliders to the training data values e.g.(1.00, 1.00). Does the output match the training data exactly? Why is that the case?\n",
    "\n",
    "**Answer:** No, we don't get the discrete values as in the training data, this is the result of weight and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "The neural network now can do something more than just predicting the values of the input set that you gave it. What \"special ability\" has your network gained automatically?\n",
    "\n",
    "**Answer:** The net can generalize and give continuous outputs to inputs it has never seen before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** \n",
    "How can this special ability be useful when applying neural networks to self-driving vehicles?\n",
    "\n",
    "**Answer:** If the model has been given enough training data, it can make safe decisions in situations it has never seen before. This way, the training data doesn't have to include every possible scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:**\n",
    "Why does this ability make it easier to use a neural network for self-driving vehicles than traditional rule-based programming\n",
    "\n",
    "**Answer:** In traditional rule-based programming you have to define every possible scenario and tell the model what to do in each case. This would be practicaly infinitly complex when it comes to traffic and therefore pretty much impossible. With this ability we don't have to show the model every possible scenario but just enough so that it can make accurate generalizations about new situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Create an OR Gate \n",
    "**Change the code above to train an OR Network and verfy your results with a test.**\n",
    "\n",
    "| Input 1| Input 2  | Output    |\n",
    "|--------|----------|-----------|\n",
    "|    0   | 0        |0          |\n",
    "|    0   | 1        |1          |\n",
    "|    1   | 0        |1          |\n",
    "|    1   | 1        |1          |\n",
    "\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "    Table. 3 - OR Truth table\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight matrix w_i:\n",
      "[[0.60454483 0.56808506 0.95642823]\n",
      " [0.79723838 0.33781668 0.88414121]]\n",
      "\n",
      "weight matrix w_o:\n",
      "[[0.22216027]\n",
      " [0.97069509]\n",
      " [0.01045813]]\n",
      "Bias\n",
      "-1.0655840409438597\n",
      "Loss after Epoch 0: 0.7052\n",
      "Loss after Epoch 5: 0.0542\n",
      "Loss after Epoch 10: 0.0246\n",
      "Loss after Epoch 15: 0.0182\n",
      "Loss after Epoch 20: 0.0150\n",
      "Loss after Epoch 25: 0.0131\n",
      "Loss after Epoch 30: 0.0117\n",
      "Loss after Epoch 35: 0.0107\n",
      "Loss after Epoch 40: 0.0099\n",
      "Loss after Epoch 45: 0.0092\n",
      "Loss after Epoch 50: 0.0087\n",
      "Loss after Epoch 55: 0.0083\n",
      "Loss after Epoch 60: 0.0079\n",
      "Loss after Epoch 65: 0.0075\n",
      "Loss after Epoch 70: 0.0072\n",
      "Loss after Epoch 75: 0.0070\n",
      "Loss after Epoch 80: 0.0067\n",
      "Loss after Epoch 85: 0.0065\n",
      "Loss after Epoch 90: 0.0063\n",
      "Loss after Epoch 95: 0.0061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\task5\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in log\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OR gate\n",
    "\n",
    "or_input_set = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "or_solution_set = np.array([[0],[1],[1],[1]])\n",
    "\n",
    "#create model\n",
    "or_gate_net = NeuralNetwork()\n",
    "initialize_network(or_gate_net) #just a test initialization to illustrate the weight matrices\n",
    "or_gate_net.show_conf()\n",
    "\n",
    "#train\n",
    "learning_rate_or = 10\n",
    "epochs_or = 100\n",
    "\n",
    "train(or_gate_net, or_input_set, or_solution_set, learning_rate_or, epochs_or)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44267400048464d8958f9457f10271f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='input1', layout=Layout(width='22%'), max=1.0, step=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def change_or(input1, input2):\n",
    "    input_vector = np.array([input1 * 1, input2 * 1])     # converting bool to float\n",
    "    prediction = or_gate_net.compute(input_vector)\n",
    "    print(\"\\t input: {} \\t \\t output: {:0.9f}\".format(input_vector, prediction[0]))\n",
    "\n",
    "interact(\n",
    "    change_or,\n",
    "    input1=FloatSlider(min=0, max=1, step=1, layout=Layout(width=\"22%\")),\n",
    "    input2=FloatSlider(min=0, max=1, step=1, layout=Layout(width=\"22%\")),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlook: Classification Tests in the Real World\n",
    "\n",
    "A classic application of neural networks is the classification of images. A commonly used data set is CIFAR-10, which consists of:  \n",
    " 1. Images of  airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks  (10 Categories)\n",
    " 2. Labels attached to each image that categorize the image\n",
    " \n",
    "<img src=\"images/cifar10_plot.png\" />\n",
    "<p style=\"text-align: center;\">\n",
    "    Fig. 3 - CIFAR-10 dataset[4]\n",
    "</p>\n",
    "\n",
    " \n",
    "The labels (also called annotations) act as the \"solution\" for the training set. Each item (airplane, car..) is a separate category. \n",
    "During training, the weights and biases in the network are adjusted in just the right way, until it performs the right mathematical operations to correctly classify the given training data. After training, the network can recognize whether the image is a cat, an airplane, etc. This even works for pictures that the network has never seen. You will find out how neural networks can perform image classification in the next class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources:\n",
    "[1] Wikipedia, Statistical classification https://en.wikipedia.org/wiki/Statistical_classification, retrieved 01.05.2019\n",
    "\n",
    "[2]  Brownlee, Jason 2018. Machine Learning Algorithms From Scratch. p. 70\n",
    "\n",
    "[3]  Gibbs, M.N. (Nov 2000). \"Variational Gaussian process classifiers\". IEEE Transactions on Neural Networks. p. 1458–1464.\n",
    "\n",
    "[4] Cifar-10, Cifar-100 Dataset Introduction\n",
    "Corochann - https://corochann.com/cifar-10-cifar-100-dataset-introduction-1258.html, retrieved 02.02.2019\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Reading\n",
    "\n",
    "The Sigmoid Function in Logistic Regression: http://karlrosaen.com/ml/notebooks/logistic-regression-why-sigmoid/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "395.996px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
